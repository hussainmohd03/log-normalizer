# -- Model Settings ------------------------------------------

MODEL_PATH=./models/phi-3-mini-base

# Device for inference. "auto" lets accelerate decide GPU/CPU.
# Options: "auto", "cpu", "cuda:0"
DEVICE=auto

# Generation temperature. 0.0 = deterministic, 1.0 = random.
# For log parsing, keep very low (0.05–0.1) for consistent JSON output.
TEMPERATURE=0.05

# Maximum tokens the model can generate per request.
# OCSF JSON typically runs 500-800 tokens. 1024 is a safe default.
MAX_NEW_TOKENS=1024

# ── Confidence Settings ------------------------------------------
# Logs scoring below this threshold go to the manual review queue.
# Range: 0.0–1.0. Default 0.85 is a reasonable starting point.
CONFIDENCE_THRESHOLD=0.85

# -- RAG Settings  ------------------------------------------
# Path where ChromaDB stores vector embeddings for example retrieval.
CHROMA_PATH=./data/chroma

# Number of similar examples to inject into the prompt via RAG.
# More examples = better accuracy but uses more of the context window.
# Range: 0–3. Default 2.
RAG_EXAMPLES_COUNT=2


# -- App Settings ------------------------------------------

# Logging level. Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
